{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_apache_spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qsquentinsmith/BigDataScience/blob/master/03_apache_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtXomYbY8ZfU"
      },
      "source": [
        "# INET 4710: Lab 03\n",
        "## Author: Quentin Smith\n",
        "### smit8819"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH5TBgEH8sAF"
      },
      "source": [
        "## 0. Installing and Running Spark on Google colab\n",
        "\n",
        "The following code will \n",
        "- Get newest versions of packages and dependencies. \n",
        "- Download jdk8 and spark 3.0.1 with hadoop 2.7\n",
        "- Download findspark (to make pyspark importable as a regular library)\n",
        "- Import os\n",
        "- Set paths for Java_Home and Spark_Home\n",
        "- Import findspark and initiate it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClJ6UJFf8Wa1",
        "outputId": "c34387fe-23c9-4051-85af-7bfba2eaee7d"
      },
      "source": [
        "# Get newest versions of packages and dependencies.\n",
        "!apt-get update\n",
        "\n",
        "# Download jdk8 and spark 3.0.1 with hadoop 2.7\n",
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://download.java.net/openjdk/jdk8u41/ri/openjdk-8u41-b04-linux-x64-14_jan_2020.tar.gz\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "\n",
        "# Download findspark (to make pyspark importable as a regular library)\n",
        "!pip install -q findspark\n",
        "\n",
        "#Import os\n",
        "import os\n",
        "\n",
        "# Set paths for Java_Home and Spark_Home\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
        "\n",
        "# Import findspark and initiate it\n",
        "import findspark\n",
        "findspark.init() "
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rHit:3 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rGet:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r0% [1 InRelease gpgv 15.9 kB] [4 InRelease 14.2 kB/88.7 kB 16%] [Connecting to \r                                                                               \rHit:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [1 InRelease gpgv 15.9 kB] [4 InRelease 35.9 kB/88.7 kB 40%] [Connecting to \r                                                                               \rHit:6 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 15.9 kB] [4 InRelease 50.4 kB/88.7 kB 57%] [Connecting to \r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [Waiting for headers] [Conn\r                                                                               \rGet:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Fetched 252 kB in 2s (150 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJXhmsYL9Tyh"
      },
      "source": [
        "### 0.1 Configuring a Spark Session\n",
        "\n",
        "The entry point to using Spark SQL is an object called SparkSession. This code will initiate a Spark Application which all the code for that session will run on. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHaTCAN589MO"
      },
      "source": [
        "# Configure Spark session and run session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "\t.master(\"local[*]\") \\\n",
        "\t.appName(\"Learning_Spark\") \\\n",
        "\t.getOrCreate() "
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eIT8kJI9jxD"
      },
      "source": [
        "## 1. Loading Data\n",
        "\n",
        "For this guide we’ll be working with a dataset on video game sales from Kaggle. It can be found here.\n",
        "\n",
        "https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOflMsBD-NlB"
      },
      "source": [
        "# Reads directly into DataFrame in PySparkSQL\n",
        "\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/qsquentinsmith/BigDataScience/master/data/Video_Games_Sales_as_at_22_Dec_2016.csv\"\n",
        "\n",
        "from pyspark import SparkFiles\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "data = spark.read.csv(\"file://\"+SparkFiles.get(\"Video_Games_Sales_as_at_22_Dec_2016.csv\"), header=True, inferSchema= True)\n",
        "\n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyKFKVZt-mAK"
      },
      "source": [
        "## 2. Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LT4uA6--RzB",
        "outputId": "5e67db99-c64f-4be3-99da-0d3433ab2503"
      },
      "source": [
        "# Shape\n",
        "data.count(), len(data.columns)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16719, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOWSr6IH_BgY",
        "outputId": "137b98a3-7f2b-44b6-846f-84b94889f727"
      },
      "source": [
        "data.show(5)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------+---------------+------------+---------+--------+--------+--------+-----------+------------+------------+------------+----------+----------+---------+------+\n",
            "|                Name|Platform|Year_of_Release|       Genre|Publisher|NA_Sales|EU_Sales|JP_Sales|Other_Sales|Global_Sales|Critic_Score|Critic_Count|User_Score|User_Count|Developer|Rating|\n",
            "+--------------------+--------+---------------+------------+---------+--------+--------+--------+-----------+------------+------------+------------+----------+----------+---------+------+\n",
            "|          Wii Sports|     Wii|           2006|      Sports| Nintendo|   41.36|   28.96|    3.77|       8.45|       82.53|          76|          51|         8|       322| Nintendo|     E|\n",
            "|   Super Mario Bros.|     NES|           1985|    Platform| Nintendo|   29.08|    3.58|    6.81|       0.77|       40.24|        null|        null|      null|      null|     null|  null|\n",
            "|      Mario Kart Wii|     Wii|           2008|      Racing| Nintendo|   15.68|   12.76|    3.79|       3.29|       35.52|          82|          73|       8.3|       709| Nintendo|     E|\n",
            "|   Wii Sports Resort|     Wii|           2009|      Sports| Nintendo|   15.61|   10.93|    3.28|       2.95|       32.77|          80|          73|         8|       192| Nintendo|     E|\n",
            "|Pokemon Red/Pokem...|      GB|           1996|Role-Playing| Nintendo|   11.27|    8.89|   10.22|        1.0|       31.37|        null|        null|      null|      null|     null|  null|\n",
            "+--------------------+--------+---------------+------------+---------+--------+--------+--------+-----------+------------+------------+------------+----------+----------+---------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ-Mox9Akic9"
      },
      "source": [
        "We see a couple things from the first two data exploration cells. The first one gives us an idea of how big the data is and how many columns we are working with. The second shows the types of most of the columns (Sometime you cannot tell the type which the next cell will show). The following cell data.printSchema() tells us that Year_of_Release although a number is actually a string in this data frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQkHsIlF_B7-",
        "outputId": "3df324b2-9380-4c15-ca3f-1c92748a9a87"
      },
      "source": [
        "# Shows column titles and types\n",
        "data.printSchema()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Platform: string (nullable = true)\n",
            " |-- Year_of_Release: string (nullable = true)\n",
            " |-- Genre: string (nullable = true)\n",
            " |-- Publisher: string (nullable = true)\n",
            " |-- NA_Sales: double (nullable = true)\n",
            " |-- EU_Sales: double (nullable = true)\n",
            " |-- JP_Sales: double (nullable = true)\n",
            " |-- Other_Sales: double (nullable = true)\n",
            " |-- Global_Sales: double (nullable = true)\n",
            " |-- Critic_Score: integer (nullable = true)\n",
            " |-- Critic_Count: integer (nullable = true)\n",
            " |-- User_Score: string (nullable = true)\n",
            " |-- User_Count: integer (nullable = true)\n",
            " |-- Developer: string (nullable = true)\n",
            " |-- Rating: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7KT2-e1_GQL",
        "outputId": "4be0e2b2-64e7-4ce6-b74c-dbb840946de5"
      },
      "source": [
        "#Choose individual columns\n",
        "data.select(\"Name\",\"Platform\",\"User_Score\",\"User_Count\").show(15, truncate=False)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------+--------+----------+----------+\n",
            "|Name                       |Platform|User_Score|User_Count|\n",
            "+---------------------------+--------+----------+----------+\n",
            "|Wii Sports                 |Wii     |8         |322       |\n",
            "|Super Mario Bros.          |NES     |null      |null      |\n",
            "|Mario Kart Wii             |Wii     |8.3       |709       |\n",
            "|Wii Sports Resort          |Wii     |8         |192       |\n",
            "|Pokemon Red/Pokemon Blue   |GB      |null      |null      |\n",
            "|Tetris                     |GB      |null      |null      |\n",
            "|New Super Mario Bros.      |DS      |8.5       |431       |\n",
            "|Wii Play                   |Wii     |6.6       |129       |\n",
            "|New Super Mario Bros. Wii  |Wii     |8.4       |594       |\n",
            "|Duck Hunt                  |NES     |null      |null      |\n",
            "|Nintendogs                 |DS      |null      |null      |\n",
            "|Mario Kart DS              |DS      |8.6       |464       |\n",
            "|Pokemon Gold/Pokemon Silver|GB      |null      |null      |\n",
            "|Wii Fit                    |Wii     |7.7       |146       |\n",
            "|Kinect Adventures!         |X360    |6.3       |106       |\n",
            "+---------------------------+--------+----------+----------+\n",
            "only showing top 15 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SbOobgp_Wgf",
        "outputId": "182e340f-91db-4faa-a985-e35c407040d9"
      },
      "source": [
        "# Take closer look at user score and user count\n",
        "data.describe([\"User_Score\",\"User_Count\"]).show()"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------------------+------------------+\n",
            "|summary|        User_Score|        User_Count|\n",
            "+-------+------------------+------------------+\n",
            "|  count|             10015|              7590|\n",
            "|   mean|7.1250461133070315|162.22990777338603|\n",
            "| stddev|1.5000060936257986| 561.2823262473789|\n",
            "|    min|                 0|                 4|\n",
            "|    max|               tbd|             10665|\n",
            "+-------+------------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjaTNunEl_MZ"
      },
      "source": [
        "We see another problem we will eventually run into. There are various null values and values listed with tbd. We will filter those out in the next session so we can run our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e6ABH_QAEmc",
        "outputId": "77bdc572-dd35-43cc-b717-0f57df1bf7ee"
      },
      "source": [
        "# Look at platform distribution\n",
        "data.groupBy(\"Platform\").count().orderBy(\"count\", ascending=False).show(10)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-----+\n",
            "|Platform|count|\n",
            "+--------+-----+\n",
            "|     PS2| 2161|\n",
            "|      DS| 2152|\n",
            "|     PS3| 1331|\n",
            "|     Wii| 1320|\n",
            "|    X360| 1262|\n",
            "|     PSP| 1209|\n",
            "|      PS| 1197|\n",
            "|      PC|  974|\n",
            "|      XB|  824|\n",
            "|     GBA|  822|\n",
            "+--------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hUo4ixBBNAw"
      },
      "source": [
        "### 2.1 Filtering DataFrames\n",
        "\n",
        "Lets create a new DataFrame that has the null values for User_Score and User_Count, and the “tbd” values filtered out using the .filter() method. This will keep the integrity of the original data frame in 'data' and our cleaned data frame 'data2'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJm26hxXU3ei",
        "outputId": "78dee8f7-cffd-4a98-f7b4-07842841e45d"
      },
      "source": [
        "# Check for nulls\n",
        "from pyspark.sql.functions import col,isnan,when,count\n",
        "df2 = data.select([count(when(col(c).contains('None') | \\\n",
        "                            col(c).contains('NULL') | \\\n",
        "                            (col(c) == '' ) | \\\n",
        "                            col(c).isNull() | \\\n",
        "                            isnan(c), c \n",
        "                           )).alias(c)\n",
        "                    for c in data.columns])\n",
        "df2.show()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+--------+---------------+-----+---------+--------+--------+--------+-----------+------------+------------+------------+----------+----------+---------+------+\n",
            "|Name|Platform|Year_of_Release|Genre|Publisher|NA_Sales|EU_Sales|JP_Sales|Other_Sales|Global_Sales|Critic_Score|Critic_Count|User_Score|User_Count|Developer|Rating|\n",
            "+----+--------+---------------+-----+---------+--------+--------+--------+-----------+------------+------------+------------+----------+----------+---------+------+\n",
            "|   6|       0|              0|    2|        1|       0|       0|       0|          0|           0|        8582|        8582|      6704|      9129|     6624|  6769|\n",
            "+----+--------+---------------+-----+---------+--------+--------+--------+-----------+------------+------------+------------+----------+----------+---------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn9L-NcIAF9x",
        "outputId": "1838a302-37cb-4483-b454-0a8ed60c20e2"
      },
      "source": [
        "#Create conditions to filter\n",
        "condition1 = (data.User_Score.isNotNull()) | (data.User_Count.isNotNull())\n",
        "condition2 = data.User_Score != \"tbd\"\n",
        "condition3 = data.Year_of_Release != 'N/A'\n",
        "condition4 = data.Critic_Score.isNotNull()\n",
        "\n",
        "\n",
        "#Apply filters. Check data\n",
        "data = data.filter(condition1).filter(condition2).filter(condition3).filter(condition4)\n",
        "data.describe([\"User_Score\",\"User_Count\", \"Year_of_Release\", \"Critic_Score\"]).show()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+------------------+------------------+------------------+------------------+\n",
            "|summary|        User_Score|        User_Count|   Year_of_Release|      Critic_Score|\n",
            "+-------+------------------+------------------+------------------+------------------+\n",
            "|  count|              6894|              6894|              6894|              6894|\n",
            "|   mean| 7.184377719756343|174.39237017696547|2007.4823034522774| 70.25848563968668|\n",
            "| stddev|1.4398056814955311| 584.8721554479154| 4.236401444992017|13.861082448459156|\n",
            "|    min|               0.5|                 4|              1985|                13|\n",
            "|    max|               9.6|             10665|              2016|                98|\n",
            "+-------+------------------+------------------+------------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UHnjP3LmaNn"
      },
      "source": [
        "# change all predictors to type Double\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# Make a copy of 'data' and name it 'data2'\n",
        "data2 = data\n",
        "\n",
        "# Change variables used in model to type: double, using cast\n",
        "data2 = data2.withColumn(\"Year_of_Release\", data2[\"Year_of_Release\"].cast(DoubleType()))\n",
        "data2 = data2.withColumn(\"User_Score\", data2[\"User_Score\"].cast(DoubleType()))\n",
        "data2 = data2.withColumn(\"User_Count\", data2[\"User_Count\"].cast(DoubleType()))\n",
        "data2 = data2.withColumn(\"Critic_Score\", data2[\"Critic_Score\"].cast(DoubleType())) "
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDvlJvZGBmYd"
      },
      "source": [
        "## 3.1 Building Models in PySpark\n",
        "\n",
        "For an example of linear regression, let’s see if we can predict User_Score from Year_of_Release, Global_Sales, Critic_Score, and User_Count.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX-vqjXym238",
        "outputId": "228dbc1f-f214-434a-8b2e-0ff3c91a63b8"
      },
      "source": [
        "# check to see change implemented\n",
        "data2.printSchema()"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Platform: string (nullable = true)\n",
            " |-- Year_of_Release: double (nullable = true)\n",
            " |-- Genre: string (nullable = true)\n",
            " |-- Publisher: string (nullable = true)\n",
            " |-- NA_Sales: double (nullable = true)\n",
            " |-- EU_Sales: double (nullable = true)\n",
            " |-- JP_Sales: double (nullable = true)\n",
            " |-- Other_Sales: double (nullable = true)\n",
            " |-- Global_Sales: double (nullable = true)\n",
            " |-- Critic_Score: double (nullable = true)\n",
            " |-- Critic_Count: integer (nullable = true)\n",
            " |-- User_Score: double (nullable = true)\n",
            " |-- User_Count: double (nullable = true)\n",
            " |-- Developer: string (nullable = true)\n",
            " |-- Rating: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RToskBHaCnFf"
      },
      "source": [
        "### 3.1 Vector Assembler\n",
        "\n",
        "Change data into new data frame that PySpark model can use by using vectorassembler. Vector Assumbly puts all the variables we use to predict the target variable (User_Score) into a vector (predictors). This vector is then stored in one column while User_Score is in the other column. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e62MbN9BB3us",
        "outputId": "7f8fed7c-5684-4e74-c714-3ff7602b04e7"
      },
      "source": [
        "# Resource Used for set handle invalid https://stackoverflow.com/questions/41362295/sparkexception-values-to-assemble-cannot-be-null.\n",
        "# Other resource: https://spark.apache.org/docs/latest/ml-features#vectorassembler\n",
        "\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# define vectorassembler\n",
        "assembler = VectorAssembler(inputCols=['Year_of_Release', 'Global_Sales', 'Critic_Score', 'User_Count'], outputCol = 'predictors')\n",
        "\n",
        "# output = assembler.setHandleInvalid(\"skip\").transform(data2)\n",
        "output = assembler.transform(data2)\n",
        "print(\"Assembled columns 'Year_of_Release', 'Global_Sales', 'Critic_Score', 'User_Count' to vector column 'predictors'\")\n",
        "\n",
        "#Collect the new predictions column and User_Score (our target variable) in a DataFrame.\n",
        "model_data = output.select([\"predictors\", \"User_Score\"])\n",
        "\n",
        "model_data.show(5, truncate = False)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Assembled columns 'Year_of_Release', 'Global_Sales', 'Critic_Score', 'User_Count' to vector column 'predictors'\n",
            "+-------------------------+----------+\n",
            "|predictors               |User_Score|\n",
            "+-------------------------+----------+\n",
            "|[2006.0,82.53,76.0,322.0]|8.0       |\n",
            "|[2008.0,35.52,82.0,709.0]|8.3       |\n",
            "|[2009.0,32.77,80.0,192.0]|8.0       |\n",
            "|[2006.0,29.8,89.0,431.0] |8.5       |\n",
            "|[2006.0,28.92,58.0,129.0]|6.6       |\n",
            "+-------------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0fxDe-zGAFj"
      },
      "source": [
        "### 3.2 Model Training\n",
        "\n",
        "We will be using Linear Regression as our model. First we need to split the model_data into test and train splits. We will use 80% of the data from 'data2' for the training group and the other 20% for the test split. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It450eLIPB1-"
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "#Split data into train and test\n",
        "train, test = model_data.randomSplit([0.8, 0.2])\n",
        "\n",
        "#Define Linear Regression model\n",
        "lr = LinearRegression(\n",
        "\tfeaturesCol = 'predictors',\n",
        "\tlabelCol = 'User_Score')\n",
        "\n",
        "#Fit the model to the training data\n",
        "lrModel = lr.fit(train)\n",
        "\n",
        "#Use evaluate class to see our prediction\n",
        "pred = lrModel.evaluate(test)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UemaE74woBUq"
      },
      "source": [
        "Now we can use seperate tools within our models to see the coefficients and the intercept to get a formula to predict future values. We can also use predictions from the class to see our predictions on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDOn0JZwVgR2",
        "outputId": "39b705c7-ba73-44e8-9cce-2c69496bb267"
      },
      "source": [
        "#access  parameters of our model: coefficients\n",
        "lrModel.coefficients"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DenseVector([-0.0768, -0.018, 0.063, -0.0002])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDCy-sDaViSW",
        "outputId": "dfc2f2fe-cdf6-4296-a2db-2eece7340875"
      },
      "source": [
        "#access  parameters of our model: intercept\n",
        "lrModel.intercept"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "156.89650865430528"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUO5DwmOVlHA",
        "outputId": "3f4e94e2-d121-4f8c-a963-1e970dc8e9d9"
      },
      "source": [
        "#View final predictions\n",
        "pred.predictions.show(5) "
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+----------+------------------+\n",
            "|          predictors|User_Score|        prediction|\n",
            "+--------------------+----------+------------------+\n",
            "|[1988.0,0.03,64.0...|       2.2|7.3952122349159595|\n",
            "|[1994.0,1.27,69.0...|       6.3| 8.175145120381075|\n",
            "|[1997.0,0.07,79.0...|       7.8| 8.596646967056756|\n",
            "|[1997.0,0.89,83.0...|       8.2| 8.830229615949833|\n",
            "|[1998.0,0.05,69.0...|       7.5| 7.890080746311099|\n",
            "+--------------------+----------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eqbUXq1VaQP"
      },
      "source": [
        "### 3.3 Model Evaluation\n",
        "\n",
        "We will use RegressionEvaluator to learn whether or not our linear regression was a good model to use on this data. There will be different metrics that we will use including: \n",
        "- Root-Mean-Square Error (RMSE)\n",
        "- Mean-Square Error (MSE)\n",
        "- Mean Absolute Error (MEA)\n",
        "- R-Squared (r2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa_QTs5rGGDh"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "#DefineRegression Evaluator, Set Parameters)\n",
        "eval = RegressionEvaluator(\n",
        "\tlabelCol=\"User_Score\",\n",
        "\tpredictionCol=\"prediction\",\n",
        "\tmetricName=\"rmse\")"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zN-WYVCRV-Bh",
        "outputId": "3a9f7ca3-1248-461a-dae5-8aaa34ccfa1d"
      },
      "source": [
        "#run different metrics\n",
        "rmse = eval.evaluate(pred.predictions)\n",
        "mse = eval.evaluate(pred.predictions, {eval.metricName: \"mse\"})\n",
        "mae = eval.evaluate(pred.predictions, {eval.metricName: \"mae\"})\n",
        "r2 = eval.evaluate(pred.predictions, {eval.metricName: \"r2\"})\n",
        "\n",
        "#print metrics\n",
        "print(\"Root-Mean-Square Error (RMSE): \", rmse)\n",
        "print(\"Mean-Square Error (MSE): \", mse)\n",
        "print(\"Mean Absolute Error (MAE): \", mae)\n",
        "print(\"R-squared (r2): \", r2)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root-Mean-Square Error (RMSE):  1.147240886673481\n",
            "Mean-Square Error (MSE):  1.3161616520553547\n",
            "Mean Absolute Error (MAE):  0.869876244720776\n",
            "R-squared (r2):  0.3832867984279704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE5wyFTrpICV"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "Our evaluation on our model Linear Regression shows the following metrics: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBr9CSW4pReF",
        "outputId": "8f10cb9e-9ae6-4ad0-d905-6b182947edab"
      },
      "source": [
        "#print metrics\n",
        "print(\"Root-Mean-Square Error (RMSE): \", rmse)\n",
        "print(\"Mean-Square Error (MSE): \", mse)\n",
        "print(\"Mean Absolute Error (MAE): \", mae)\n",
        "print(\"R-squared (r2): \", r2)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root-Mean-Square Error (RMSE):  1.147240886673481\n",
            "Mean-Square Error (MSE):  1.3161616520553547\n",
            "Mean Absolute Error (MAE):  0.869876244720776\n",
            "R-squared (r2):  0.3832867984279704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDTN9Q_jp99Y"
      },
      "source": [
        "**Root-Mean-Square Error (RMSE)** is the standard deviation of the prediction errors. It tells you how concentrated the data is around the line of best fit. So the prediction errors (residuals) have a standard deviation of 1.15. \n",
        "\n",
        "**Mean-Square Error (MSE)** tells you how close a regression line is to a set of points. The smaller the mean squared error, the closer you are to finding the line of best fit. The MSE is 1.32.\n",
        "\n",
        "**Mean Absolute Error (MAE)** is mean of all the difference between the measured value and 'true' value. So the mean difference between our model predicted values and the true value is 0.87.\n",
        "\n",
        "**R-Squared (r2)** also called Coefficient of Determination. It is used to analyze how difference in one variable can be explained by a difference in a second variable. The percentage indicates that about 38% of predicted points should fall within the regression. Other things this let's us know is how much of a connection there is between the target variable and the predictors. \n",
        "\n",
        "For a first try this is a good start. These perdictor variables do have some connection to the target varaible. The next steps we should take is using a feature selection method so we can determine the best predictor variables. We should also use cross validation to fight the bit of overfitting that might be happening. \n",
        "\n",
        "\n",
        "**Resource:**\n",
        "\n",
        "https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/\n",
        "\n",
        "https://www.statisticshowto.com/mean-squared-error/\n",
        "\n",
        "https://www.statisticshowto.com/absolute-error/\n",
        "\n",
        "https://www.statisticshowto.com/probability-and-statistics/coefficient-of-determination-r-squared/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0kCVCgxVFa-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}